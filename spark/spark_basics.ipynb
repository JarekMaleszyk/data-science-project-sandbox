{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1282b669-e2c0-4609-98f8-f0cf396118eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import pyspark\n",
    "except:\n",
    "  !pip install pyspark\n",
    "  import pyspark\n",
    "finally:\n",
    "  from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "85522500-356c-403b-9e4b-fcc6cf51f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def create_folder(directory_name: str) -> None:\n",
    "    try:\n",
    "        os.mkdir(Path(directory_name))\n",
    "        print(f\"Directory '{directory_name}' created successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Directory '{directory_name}' already exists.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def remove_file(directory_name: str, file_name: str) -> None:\n",
    "    try:\n",
    "        os.remove(f\"{Path(directory_name)}/{file_name}\") \n",
    "        print(f\"File '{Path(directory_name)}/{file_name}' removed successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"File '{Path(directory_name)}/{file_name}' doesn't exists.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to remove file '{Path(directory_name)}/{file_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a46e87af-6124-4258-b577-7a53a49e188d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './tmp/' already exists.\n",
      "File 'tmp/electro_product_mock.json' removed successfully.\n",
      "100% [................................................................................] 1537 / 1537"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data.json'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "try:\n",
    "  import wget\n",
    "except:\n",
    "  !pip install wget -q\n",
    "  import wget\n",
    "\n",
    "try:\n",
    "  import shutil\n",
    "except:\n",
    "  !pip install shutil -q\n",
    "  import shutil\n",
    "\n",
    "FILE_PATH = \"./tmp/\"\n",
    "FILE_NAME = \"electro_product_mock.json\"\n",
    "URL = \"https://raw.githubusercontent.com/JarekMaleszyk/data-science-project-sandbox/refs/heads/main/data.json\"\n",
    "\n",
    "if not os.path.isfile(FILE_PATH):\n",
    "    create_folder(FILE_PATH)\n",
    "    remove_file(FILE_PATH, FILE_NAME) \n",
    "    filename = wget.download(URL)\n",
    "    shutil.move(filename, f\"{Path(FILE_PATH)}/{FILE_NAME}\")\n",
    "\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "58187776-921c-4c01-a0e1-7ca8564c32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName('Spark basics').getOrCreate()\n",
    "sparkSession.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b1fc077e-f842-4e0e-8145-cd5b043a6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType,\n",
    "                               ArrayType, DoubleType, BooleanType)\n",
    "\n",
    "schema = StructType(fields=[\n",
    "    StructField(\"id\", IntegerType(), True), # True = nullable\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74b7fdc6-6a26-4e54-8adb-6c1efebd89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_df_products = sparkSession.read\\\n",
    "  .schema(schema)\\\n",
    "  .option(\"multiline\", True)\\\n",
    "  .json(f\"{Path(FILE_PATH)}/{FILE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e640777b-7e30-4fe8-ba87-53a19a8c4dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pyspark_df_products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "517655a5-bc9f-45a0-ae54-658bb7cc9120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "|  6|            Yoga Mat|         Sports|      30| 29.99|\n",
      "|  7| Samsung 4K Smart TV|    Electronics|       8|799.99|\n",
      "|  8|        Levi's Jeans|       Clothing|      15| 49.99|\n",
      "|  9|Dyson Vacuum Cleaner|Home Appliances|       3|399.99|\n",
      "| 10| Harry Potter Series|          Books|      20| 15.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pyspark_df_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "79eee112-9f48-412a-bf7f-2db5ba4329ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utworzenie danych.\n",
    "newDataFramePayload = [\n",
    "           [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "           [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "           [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "           [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "           [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "           [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f7bff11d-8d15-41ff-9ea8-22504ae0ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, ArrayType\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "# Zdefiniowanie schematu dla danych.\n",
    "newDataFrameSchema = StructType([\n",
    "   StructField(\"Id\", IntegerType(), False),\n",
    "   StructField(\"First\", StringType(), False),\n",
    "   StructField(\"Last\", StringType(), False),\n",
    "   StructField(\"Url\", StringType(), False),\n",
    "   StructField(\"Published\", StringType(), False),\n",
    "   StructField(\"Hits\", IntegerType(), False),\n",
    "   StructField(\"Campaigns\", ArrayType(StringType()), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "00648e93-594d-47bf-8944-b353f8d97880",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkValueError",
     "evalue": "[LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 7 and 5.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Utworzenie egzemplarza DataFrame za pomocą zdefiniowanego wcześniej schematu.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m blogs_df \u001b[38;5;241m=\u001b[39m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Wyświetlenie egzemplarza DataFrame. Powinien on odzwierciedlać przedstawioną wcześniej tabelę.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m blogs_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m-> 1459\u001b[0m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/types.py:2201\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2201\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/types.py:2164\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m   2163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(verifiers):\n\u001b[0;32m-> 2164\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2165\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2166\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   2167\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2168\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfields\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2169\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg1_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(obj)),\n\u001b[1;32m   2170\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg2_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(verifiers)),\n\u001b[1;32m   2171\u001b[0m             },\n\u001b[1;32m   2172\u001b[0m         )\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[1;32m   2174\u001b[0m         verifier(v)\n",
      "\u001b[0;31mPySparkValueError\u001b[0m: [LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 7 and 5."
     ]
    }
   ],
   "source": [
    "# Utworzenie egzemplarza DataFrame za pomocą zdefiniowanego wcześniej schematu.\n",
    "blogs_df = sparkSession.createDataFrame(data, schema)\n",
    "# Wyświetlenie egzemplarza DataFrame. Powinien on odzwierciedlać przedstawioną wcześniej tabelę.\n",
    "blogs_df.show()\n",
    "\n",
    "# Poza jupyter-lab mozna napisać to jako program główny main.py \n",
    "\n",
    "# # Program główny.\n",
    "# if __name__ == \"__main__\":\n",
    "#    # Utworzenie egzemplarza SparkSession.\n",
    "#    spark = (SparkSession\n",
    "#        .builder\n",
    "#        .appName(\"Example-3_6\")\n",
    "#        .getOrCreate())\n",
    "#    # Utworzenie egzemplarza DataFrame za pomocą zdefiniowanego wcześniej schematu.\n",
    "#    blogs_df = spark.createDataFrame(data, schema)\n",
    "#    # Wyświetlenie egzemplarza DataFrame. Powinien on odzwierciedlać przedstawioną wcześniej tabelę.\n",
    "#    blogs_df.show()\n",
    "\n",
    "# Następnie uruchamiać: spark-submit main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741b6ed-a722-4867-a17c-db68277f50eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36260b9-1610-4c81-b34e-d7ed4dc0d1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21497fb-8e3e-4402-91c3-1ce66b3e33bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a35fd-e7b4-4efc-9665-7c6b337f5941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c28d72-9467-42b8-bc4e-ff1a66c69da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
